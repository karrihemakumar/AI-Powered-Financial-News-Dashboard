# AI-Powered-Financial-News-Dashboard

ğŸ“˜ Detailed Project Overview
ğŸ¯ Objective
The goal of this project is to build an AI-powered financial news analysis platform using local language models. The application helps users analyze, summarize, and interact with live financial news using custom queries, without depending on cloud-based LLM APIs like OpenAI. The focus is on real-time insights, data privacy, and custom interpretability.

ğŸ—ï¸ Core Architecture
1. News Extraction (via Finnhub API)
Pulls real-time financial news categorized by:

ğŸ“° General market news

ğŸ¤ Mergers & acquisitions

ğŸ’± Forex movements

ğŸ“‰ Company earnings and more

Each news item includes:

Headline

Source

Sentiment (positive/neutral/negative)

2. Local LLM Inference (via Ollama + MCP Server)
Uses Ollama to load lightweight LLMs locally (e.g., Mistral, LLaMA2).

An MCP server acts as an intermediary between the UI and the local model:

Accepts structured prompts (context + user question)

Returns clean, contextual analysis or summaries

The LLM understands:

Full news context

Custom questions

Prompt templates for market commentary

3. Streamlit Interface
Fast, lightweight frontend to:

Display categorized news

Visualize sentiment via Plotly pie chart

Accept user queries (e.g., â€œHow will todayâ€™s earnings reports affect the S&P 500?â€)

Show LLM-generated summaries and predictions

ğŸ’¡ Unique Capabilities
Feature	Description
ğŸ“¡ Real-time News Fetching	Live market news from Finnhub categorized by type
ğŸ§  Local LLM Summarization	Uses a local model (via Ollama) to avoid cloud API cost & latency
ğŸ’¬ Custom Query Support	User can ask any finance-related question based on news context
ğŸ” Offline & Privacy-Safe	Entire pipeline runs locally (great for internal tools or secure setups)
ğŸ“Š Sentiment Visualization	Pie charts to show market tone from latest headlines
ğŸ§© Modular Backend Design	Code is separated into news_extractor.py, Streamlit UI, and LLM bridge

ğŸ§± Tech Stack
Layer	Tools/Libraries
ğŸ–¼ï¸ Frontend	Streamlit, Plotly
ğŸ”— API Client	Finnhub API
ğŸ”„ Middleware	MCP Server (custom request/response)
ğŸ¤– LLM	Ollama running local model (Mistral, LLaMA2, etc.)
ğŸ Language	Python (3.10+)

ğŸ“ˆ Example Use Cases
ğŸ§¾ Quick summary: â€œSummarize todayâ€™s top financial news.â€

ğŸ“‰ Sentiment insight: â€œWhat is the general market tone today?â€

ğŸ“Š Impact prediction: â€œWhat effect could merger news have on the energy sector?â€

ğŸ§  Trend recognition: â€œAre there recurring themes in todayâ€™s global market news?â€

ğŸ” Benefits of Using Local LLMs
âœ… Works offline / on-premise

âœ… No rate limits or token costs

âœ… Full control over model behavior and customization

âœ… Higher privacy for sensitive financial queries


